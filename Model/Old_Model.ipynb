{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera version del modelo sin actualizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/jose/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from Data_Paths_Model import *\n",
    "import numpy as np\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudiosDataset(Dataset):\n",
    "    def __init__(self, labels_df, dataset_df, image_sizes=(220, 220), n_fft=2048, hop_length=512):\n",
    "        \"\"\"\n",
    "        Initialize the Audios Dataset class.\n",
    "\n",
    "        Args:\n",
    "            labels_df (pandas.DataFrame): DataFrame containing audio labels.\n",
    "            dataset_df (pandas.DataFrame): DataFrame containing audio files and their corresponding Mel spectrograms.\n",
    "            image_sizes (tuple, optional): Tuple containing the size of the image (height, width). Default is (220, 220).\n",
    "            n_fft (int, optional): Number of samples per frame for Fourier Transform. Default is 2048.\n",
    "            hop_length (int, optional): Number of samples between frames. Default is 512.\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df\n",
    "        self.dataset_df = dataset_df\n",
    "        self.image_sizes = image_sizes\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.normalize = transforms.Normalize(mean=0, std=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the Mel spectrogram and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Get the audio path and label from the labels DataFrame\n",
    "        audio_path = self.labels_df.loc[idx, 'audio_path']\n",
    "        label = self.labels_df.loc[idx, 'forgery_type_index']\n",
    "        \n",
    "        # Find the corresponding row in the dataset DataFrame\n",
    "        dataset_row = self.dataset_df[self.dataset_df['audio_path'] == audio_path].iloc[0]\n",
    "        \n",
    "        # Extract the Mel spectrogram\n",
    "        mel_spectrogram_str = dataset_row['spectogram_of_audio']\n",
    "        mel_spectrogram = np.array(eval(mel_spectrogram_str)) \n",
    "        \n",
    "        # Convert the NumPy array to a PyTorch tensor\n",
    "        mel_spectrogram_tensor = torch.tensor(mel_spectrogram)  \n",
    "        \n",
    "        # Resize the images\n",
    "        resize = T.Resize(self.image_sizes)\n",
    "        mel_spectrogram_resized = resize(mel_spectrogram_tensor)\n",
    "\n",
    "        # Normalize the spectrogram data\n",
    "        mel_spectrogram_tensor = self.normalize(mel_spectrogram_resized)\n",
    "\n",
    "        # Apply data augmentation\n",
    "        # aug_mel_spectrogram = self.spectro_augment(mel_spectrogram_resized)\n",
    "        \n",
    "        return mel_spectrogram_tensor, label\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1, noise_std=0.1):\n",
    "        \"\"\"\n",
    "        SpecAugment: data augmentation technique for spectrograms. \n",
    "        It applies frequency and time masks to the spectrogram to avoid overfitting and help the model generalize better. \n",
    "        the model to generalize better. This data augmentation process helps to improve the performance and generalizability of the spectrogram. \n",
    "        performance and generalization capability of convolutional neural network models trained on audio data. \n",
    "        trained on audio data.\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor): Input spectrogram.\n",
    "            max_mask_pct (float, optional): Maximum percentage of the spectrogram to mask.\n",
    "            n_freq_masks (int, optional): Number of frequency masks to apply.\n",
    "            n_time_masks (int, optional): Number of time masks to apply.\n",
    "            noise_std (float, optional): Standard deviation of the Gaussian noise to add.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Augmented spectrogram.\n",
    "        \"\"\"\n",
    "        aug_spec = spec\n",
    "\n",
    "        # Apply frequency masking\n",
    "        freq_mask_param = int(max_mask_pct * aug_spec.size(1))\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(aug_spec)\n",
    "\n",
    "        # Apply time masking\n",
    "        time_mask_param = int(max_mask_pct * aug_spec.size(2))\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(aug_spec)\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        noise = torch.randn_like(aug_spec) * noise_std\n",
    "        aug_spec = aug_spec + noise\n",
    "\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels and audios DataFrame\n",
    "labels_df = pd.read_csv(LABELS_DATASET_PATH)\n",
    "dataset_df = pd.read_csv(AUDIOS_DATASET_PATH)\n",
    "\n",
    "# Create the Sound Dataset instance\n",
    "sound_dataset = AudiosDataset(labels_df, dataset_df)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_size = int(0.8 * len(sound_dataset))\n",
    "test_size = len(sound_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(sound_dataset, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some standard values for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jose/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[13], line 16\u001b[0m\n    for inputs, labels in train_loader:\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m in \u001b[1;35m__next__\u001b[0m\n    data = self._next_data()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m in \u001b[1;35m_next_data\u001b[0m\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m in \u001b[1;35mfetch\u001b[0m\n    data = self.dataset.__getitems__(possibly_batched_index)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:399\u001b[0m in \u001b[1;35m__getitems__\u001b[0m\n    return [self.dataset[self.indices[idx]] for idx in indices]\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/src/TFG/LabelDataset/DeepLearningForgeryAudioDetection/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:399\u001b[0m in \u001b[1;35m<listcomp>\u001b[0m\n    return [self.dataset[self.indices[idx]] for idx in indices]\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 44\u001b[0;36m in \u001b[0;35m__getitem__\u001b[0;36m\n\u001b[0;31m    mel_spectrogram = np.array(eval(mel_spectrogram_str))\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [[[1.1877217e-05 5.5631885e-05 7.3132942e-05 ... 3.1958414e-05\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Define the pretrained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the last layer to have 3 output classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
