{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainning\n",
    "\n",
    "**Autores**: \n",
    "\n",
    "José Antonio Ruiz Heredia (josrui05@ucm.es) \n",
    "\n",
    "Néstor Marín \n",
    "\n",
    "**Descripción**: \n",
    "En este código realizamos experimentos para el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias y las rutas de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/src/TFG/AudioForgeryDetection/tfg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DataPaths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDataPaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_collate\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DataPaths'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR,CosineAnnealingWarmRestarts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import random\n",
    "from torchaudio_augmentations import (\n",
    "    Compose,\n",
    "    LowPassFilter,\n",
    "    HighLowPass,\n",
    "    Noise,\n",
    "    PitchShift,\n",
    "    RandomApply,\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import timm\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from Data_Paths_Model import *\n",
    "from torch.utils.data import default_collate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchaudio.transforms import Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PARAMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Hiper-parametrso de Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESPEC = 'mfcc'\n",
    "ESPEC = 'mel'\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 60 # 20 seconds\n",
    "N_FFT = 1024\n",
    "PRETRAINED = True\n",
    "N = 30\n",
    "\n",
    "if N_FFT == 2048:\n",
    "    TIME_MASK_PARAM =10\n",
    "    FREQ_MASK_PARAM = 5\n",
    "    HOP_LENGTH = 1024\n",
    "else:\n",
    "    TIME_MASK_PARAM = 10\n",
    "    FREQ_MASK_PARAM = 5\n",
    "    HOP_LENGTH = 512\n",
    "\n",
    "        \n",
    "N_MELS = 224 #128\n",
    "N_MFCC = N_MELS\n",
    "N_LFCC = N_MELS\n",
    "RESIZE = False\n",
    "IMG_SIZE = 224 #384, #300 , 240\n",
    "NUM_CLASSES = 3\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "num_samples = MAX_DURATION * SAMPLE_RATE\n",
    "exp_num = '43_mel_mix_up_cv'\n",
    "\n",
    "MODEL_NAME = 'convnextv2_femto.fcmae_ft_in1k' # tf_efficientnetv2_b0.in1k,tf_efficientnetv2_b3.in21k_ft_in1k (240x240),tf_efficientnetv2_s.in21k_ft_in1k (300x300),\n",
    "#convnextv2_nano.fcmae_ft_in22k_in1k, convnextv2_femto.fcmae_ft_in1k, convnextv2_nano.fcmae_ft_in22k_in1k_384\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Hiper-parametros de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1E-4\n",
    "scheduler_learning = True\n",
    "weight_decay = 1E-2\n",
    "\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 10\n",
    "label_smoothing = 0.1\n",
    "STEP_TO_DECAY = 1\n",
    "GAMMA_TO_DECAY = 0.95\n",
    "WEIGHT = True\n",
    "f_weighted = 1.5\n",
    "AUG= True\n",
    "\n",
    "HIDDEN_UNITS=[256,128, 64]\n",
    "DROPOUT_RATE = 0.5\n",
    "SEED = 42\n",
    "SEED_AUG = 1234\n",
    "exp_name = f\"CNNCustom_exp_{exp_num}_{MAX_DURATION}s_{ESPEC}_N_FFT_{N_FFT}_MODEL_{MODEL_NAME}_AUG_{AUG}_WEIGHT_{WEIGHT}_NMELS_{N_MELS}\"\n",
    "\n",
    "torch.manual_seed(SEED_AUG)\n",
    "# Establecer la semilla para Numpy\n",
    "np.random.seed(SEED_AUG)\n",
    "# Establecer la semilla para Python random\n",
    "random.seed(SEED_AUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Parametros para el aumento del audio (ADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug_transforms = [\n",
    "                    RandomApply([Noise(min_snr=0.05, max_snr=0.1),],p=0.5,),\n",
    "                    RandomApply([PitchShift(sample_rate=SAMPLE_RATE, n_samples=num_samples, pitch_shift_max=3, pitch_shift_min=-3),],p=0.3,),\n",
    "                    #RandomApply([HighLowPass(sample_rate=SAMPLE_RATE) ],p=0.3,) \n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_acc = MulticlassAccuracy(device=device)\n",
    "metric_f1 = MulticlassF1Score(device=device, num_classes=NUM_CLASSES, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Parametros finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = f\"from_scratch_exp_{exp_num}\"\n",
    "params = {'epochs': num_epochs, 'lr': learning_rate, 'weight_decay': weight_decay, \n",
    "          'batch_size': batch_size, 'early_stopping_patience': early_stopping_patience,\n",
    "          'label_smoothing': label_smoothing, 'num_classes': NUM_CLASSES, 'n_fft': N_FFT,\n",
    "          'hop_length': HOP_LENGTH,'n_mels': N_MELS, 'max_duration': MAX_DURATION, \n",
    "          'exp_name': exp_name, 'exp_num': exp_num, 'n_layers': N, 'augmented': AUG, 'hidden_units': HIDDEN_UNITS, \n",
    "          'audios_max_duration': MAX_DURATION, 'espectogram':ESPEC, 'Freq_Mask': FREQ_MASK_PARAM, \n",
    "          'Time_Mask': TIME_MASK_PARAM, 'scheduler': scheduler_learning, 'dropout_rate': DROPOUT_RATE, 'model_name': MODEL_NAME\n",
    "          , 'weighted': WEIGHT, 'img_size': IMG_SIZE, 'f_weighted': f_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.audio_df = annotations_file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_df.iloc[idx]['audio_path']\n",
    "        audio = torchaudio.load(audio_path, normalize=True)\n",
    "        label = self.audio_df.iloc[idx]['forgery_type_index']\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return audio, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio_duration(audio, sample_rate):\n",
    "    if sample_rate != 16000: \n",
    "        resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        audio = resampler(audio)\n",
    "\n",
    "    # Calculate target length based on 5 seconds\n",
    "    target_length = 5 * sample_rate\n",
    "\n",
    "    # Trim or pad the audio to match the target length\n",
    "    current_length = audio.size(1)\n",
    "    if current_length < target_length:\n",
    "        padding_left = (target_length - current_length) // 2\n",
    "        padding_right = target_length - current_length - padding_left\n",
    "        audio = torch.nn.functional.pad(audio, (padding_left, padding_right))\n",
    "    elif current_length > target_length:\n",
    "        start_idx = (current_length - target_length) // 2\n",
    "        audio = audio[:, start_idx:start_idx+target_length]\n",
    "\n",
    "    return audio, sample_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mfcc_aug(audio):\n",
    "    # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    WAVEFORM, SAMPLE_RATE = normalize_audio_duration(WAVEFORM, SAMPLE_RATE)\n",
    "\n",
    "    c_transform = Compose(aug_transforms)\n",
    "    transformed_audio = c_transform(WAVEFORM)\n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - transformed_audio.shape[1]\n",
    "    \n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(transformed_audio, (0, padding))\n",
    "\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        log_mels=False,\n",
    "        melkwargs={\n",
    "        'n_fft':N_FFT,\n",
    "        'win_length':None,\n",
    "        'hop_length':HOP_LENGTH,\n",
    "        'center':True,\n",
    "        'pad_mode':\"reflect\",\n",
    "        'power':2.0,\n",
    "        'norm':\"slaney\",\n",
    "        'n_mels':N_MELS,\n",
    "        'mel_scale':\"htk\" \n",
    "        })\n",
    "\n",
    "        \n",
    "    mfcc_spectrogram = mfcc_transform(waveform_padding)\n",
    "    \n",
    "    time_masking = T.TimeMasking(time_mask_param=TIME_MASK_PARAM)\n",
    "    freq_masking = T.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)\n",
    "\n",
    "    time_masked = time_masking(mfcc_spectrogram)\n",
    "    freq_masked = freq_masking(time_masked)\n",
    "    \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_mfcc_spectrogram= torchaudio.transforms.AmplitudeToDB()(freq_masked)\n",
    "    if log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min() != 0:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min()) / (log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min())\n",
    "    else:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min())  \n",
    "        \n",
    "    mfcc_spectrogram_norm =mfcc_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    mfcc_spectrogram_norm_rgb = mfcc_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "    mfcc_spectrogram_norm_rgb_resize = resize_image(mfcc_spectrogram_norm_rgb)/255\n",
    "    mfcc_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    mfcc_imagenet = mfcc_normalize(mfcc_spectrogram_norm_rgb_resize)\n",
    "    return mfcc_imagenet\n",
    "\n",
    "\n",
    "def preprocess_mfcc(audio):\n",
    "    # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    WAVEFORM, SAMPLE_RATE = normalize_audio_duration(WAVEFORM, SAMPLE_RATE)\n",
    "    \n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - WAVEFORM.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n",
    "\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        log_mels=False,\n",
    "        melkwargs={\n",
    "        'n_fft':N_FFT,\n",
    "        'win_length':None,\n",
    "        'hop_length':HOP_LENGTH,\n",
    "        'center':True,\n",
    "        'pad_mode':\"reflect\",\n",
    "        'power':2.0,\n",
    "        'norm':\"slaney\",\n",
    "        'n_mels':N_MELS,\n",
    "        'mel_scale':\"htk\" \n",
    "        })\n",
    "\n",
    "        \n",
    "    mfcc_spectrogram = mfcc_transform(waveform_padding)\n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_mfcc_spectrogram= torchaudio.transforms.AmplitudeToDB()(mfcc_spectrogram)\n",
    "    if log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min() != 0:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min()) / (log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min())\n",
    "    else:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min())  \n",
    "        \n",
    "    mfcc_spectrogram_norm =mfcc_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    mfcc_spectrogram_norm_rgb = mfcc_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "    mfcc_spectrogram_norm_rgb_resize = resize_image(mfcc_spectrogram_norm_rgb)/255\n",
    "    mfcc_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    mfcc_imagenet = mfcc_normalize(mfcc_spectrogram_norm_rgb_resize)\n",
    "    return mfcc_imagenet\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espectograma de mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_train_aug(audio):\n",
    "    # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    WAVEFORM, SAMPLE_RATE = normalize_audio_duration(WAVEFORM, SAMPLE_RATE)\n",
    "    \n",
    "    # Data augmentation\n",
    "    c_transform = Compose(aug_transforms)\n",
    "    transformed_audio = c_transform(WAVEFORM)\n",
    "    \n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - transformed_audio.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(transformed_audio, (0, padding))\n",
    "    \n",
    "    n_fft = N_FFT\n",
    "    win_length = None\n",
    "    hop_length = HOP_LENGTH\n",
    "    n_mels = N_MELS\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(waveform_padding)\n",
    "    #T.TimeStretch()\n",
    "\n",
    "    time_masking = T.TimeMasking(time_mask_param=TIME_MASK_PARAM)\n",
    "    freq_masking = T.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)\n",
    "    \n",
    "\n",
    "    time_masked = time_masking(melspec)\n",
    "    freq_masked = freq_masking(time_masked)\n",
    "        \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(freq_masked)\n",
    "    log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(freq_masked)\n",
    "    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n",
    "    else:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())  \n",
    "    log_mel_spectrogram_norm =log_mel_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    if  RESIZE: \n",
    "        resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "\n",
    "        log_mel_spectrogram_norm_rgb = resize_image(log_mel_spectrogram_norm_rgb)/255\n",
    "    else:\n",
    "        log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb/255\n",
    "    log_mel_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    log_mel_imagenet = log_mel_normalize(log_mel_spectrogram_norm_rgb)\n",
    "    return log_mel_imagenet\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(audio):\n",
    "    # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    WAVEFORM, SAMPLE_RATE = normalize_audio_duration(WAVEFORM, SAMPLE_RATE)\n",
    "    \n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - WAVEFORM.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n",
    "    \n",
    "    n_fft = N_FFT\n",
    "    win_length = None\n",
    "    hop_length = HOP_LENGTH\n",
    "    n_mels = N_MELS\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(waveform_padding)\n",
    "    \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n",
    "    else:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())  \n",
    "    log_mel_spectrogram_norm =log_mel_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "    if  RESIZE: \n",
    "        log_mel_spectrogram_norm_rgb = resize_image(log_mel_spectrogram_norm_rgb)/255\n",
    "    else:\n",
    "        log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb/255\n",
    "    log_mel_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    log_mel_imagenet = log_mel_normalize(log_mel_spectrogram_norm_rgb)\n",
    "    return log_mel_imagenet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, dropout_rate):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # Agregar la primera capa oculta con la entrada original\n",
    "        self.layers.append(torch.nn.Linear(input_size, hidden_units[0]))\n",
    "        self.layers.append(torch.nn.BatchNorm1d(hidden_units[0]))\n",
    "        self.layers.append(torch.nn.ReLU())\n",
    "        self.layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Agregar el resto de las capas ocultas\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            self.layers.append(torch.nn.Linear(hidden_units[i], hidden_units[i+1]))\n",
    "            self.layers.append(torch.nn.BatchNorm1d(hidden_units[i+1]))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        self.layers.append(torch.nn.Linear(hidden_units[-1], NUM_CLASSES))\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CustomConvNeXt(nn.Module):\n",
    "    def __init__(self, N=0):\n",
    "        super(CustomConvNeXt, self).__init__()\n",
    "        \n",
    "        # Cargar el modelo preentrenado\n",
    "        self.pretrained_model  = timm.create_model(MODEL_NAME, pretrained=PRETRAINED, num_classes=0, global_pool='avg') #convnextv2_femto.fcmae_ft_in1k.fcmae_ft_in22k_in1k_384 \n",
    "        #timm.create_model('convnextv2_nano.fcmae_ft_in22k_in1k', pretrained=True, num_classes=0)\n",
    "        self.n_layers = N\n",
    "        # Congelar todas las capas primero\n",
    "        for name, param in self.pretrained_model.named_parameters():\n",
    "           param.requires_grad = False\n",
    "\n",
    "        # Descongelar las últimas 20 capas que no son BatchNormalization\n",
    "        unfrozen_count = 0\n",
    "        for name, param in reversed(list(self.pretrained_model.named_parameters())):\n",
    "            if 'bn' not in name and unfrozen_count < self.n_layers:\n",
    "                param.requires_grad = True\n",
    "                unfrozen_count += 1\n",
    "        \n",
    "        \n",
    "        #self.add_vit = self.pretrained_model.num_features\n",
    "        self.additional_layer = MLPModel(self.pretrained_model.num_features, hidden_units=HIDDEN_UNITS, dropout_rate= DROPOUT_RATE)\n",
    "        #self.additional_layer = torch.nn.Linear(self.pretrained_model.num_features, NUM_CLASSES)\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        #x = self.avgpool(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\n",
    "        x = self.additional_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_binary(labels):\n",
    "    # Aplicar un umbral de 0.5\n",
    "    binary_labels = torch.argmax(labels, dim=1)\n",
    "    return binary_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, epoch, train_loader, criterion, optimizer, scheduler=None):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss_train = 0.0\n",
    "    # Iterate over the training data (forward pass and backward pass)\n",
    "    tepoch = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\")\n",
    "    for inputs, labels in tepoch:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #feed forward\n",
    "        loss = criterion(outputs, labels) #calculate the loss\n",
    "        loss.backward()#backward pass to calculate the gradients\n",
    "        optimizer.step()#update the weights\n",
    "        running_loss_train += loss.item() * inputs.size(0) #accumulate the loss\n",
    "        # metric.update() updates the metric state with new data\n",
    "        \n",
    "        binary_labels = convert_labels_to_binary(labels)\n",
    "        metric_acc.update(outputs, binary_labels)\n",
    "        metric_f1.update(outputs, binary_labels)\n",
    "        tepoch.set_postfix(loss=loss.item(), acc=metric_acc.compute().item(), f1=metric_f1.compute().item())\n",
    "        \n",
    "    if scheduler and (epoch+1) % STEP_TO_DECAY == 0:\n",
    "        scheduler.step()\n",
    "    train_epoch_loss = running_loss_train / len(train_loader.dataset)\n",
    "    train_acc = metric_acc.compute().item()\n",
    "    train_f1 = metric_f1.compute().item()\n",
    "    return train_epoch_loss, train_acc, train_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenamiento\n",
    "def train_model(fold, model, train_loader, val_loader, criterion, criterion_val,optimizer, scheduler=None, num_epochs=10, save_path=None, early_stopping_patience=3, min_delta = 0.001, val = 'val'):\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    patience = early_stopping_patience\n",
    "\n",
    "    #for epoch in range(num_epochs):\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        # Call the function\n",
    "        train_epoch_loss, train_acc,train_f1 =  train_epoch(model, epoch, train_loader, criterion, optimizer, scheduler)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}, Fold: {fold}, Loss: {train_epoch_loss:.4f},  Acc: {train_acc:.4f}, F1-Macro: {train_f1:.4f}\")\n",
    "\n",
    "        # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # evaluation step\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        running_loss_val= 0.0\n",
    "     \n",
    "        tepoch = tqdm(val_loader, desc=f\"Val {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in tepoch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs) #feed forward\n",
    "            loss = criterion_val(outputs, labels) #calculate the loss\n",
    "            running_loss_val += loss.item() * inputs.size(0) #accumulate the loss\n",
    "\n",
    "            # metric.update() updates the metric state with new data\n",
    "            metric_acc.update(outputs, labels)\n",
    "            metric_f1.update(outputs, labels)\n",
    "            tepoch.set_postfix(val_loss=loss.item(), val_acc=metric_acc.compute().item(), val_f1=metric_f1.compute().item())\n",
    "\n",
    "        val_epoch_loss = running_loss_val / len(val_loader.dataset)\n",
    "        val_acc = metric_acc.compute().item()\n",
    "        val_f1 = metric_f1.compute().item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Fold: {fold}, Val_loss: {val_epoch_loss:.4f},  Val_acc: {val_acc:.4f}, Val_f1-Macro: {val_f1:.4f}\")\n",
    "          # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        \n",
    "\n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # Guardar el modelo si la pérdida es la mejor hasta ahora\n",
    "        if (val_f1 - best_f1) > min_delta:\n",
    "            print(\"Improved val F1 from {:.4f} to {:.4f}. Saving model...\".format(best_f1, val_f1))\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_epoch_loss,\n",
    "                'val_loss': val_epoch_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'val_f1': val_f1,\n",
    "                'train_f1': train_f1,\n",
    "            }, save_path)\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "            patience = early_stopping_patience\n",
    "            \n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "    print('Finished Training')\n",
    "    dict_model = torch.load(save_path)\n",
    "    dict_results = {'epoch':dict_model['epoch'],'train_loss': dict_model['train_loss'], 'val_loss':dict_model['val_loss'], 'train_acc':dict_model['train_acc'],\n",
    "                    'val_acc':dict_model['val_acc'], 'val_f1':dict_model['val_f1'], 'train_f1':dict_model['train_f1']}\n",
    "   # model = model.load_state_dict(dict_model['model_state_dict'])\n",
    "    return  dict_results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_for_testing( live, model, train_loader, test_loader, criterion, criterion_test,optimizer, scheduler=None, num_epochs=10, save_path=None, \n",
    "                early_stopping_patience=3, min_delta = 0.003, val = 'val'):\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    patience = early_stopping_patience\n",
    "\n",
    "    #for epoch in range(num_epochs):\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        # Call the function\n",
    "        train_epoch_loss, train_acc,train_f1 =  train_epoch(model, epoch, train_loader, criterion, optimizer, scheduler)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}, Loss: {train_epoch_loss:.4f},  Acc: {train_acc:.4f}, F1-Macro: {train_f1:.4f}\")\n",
    "\n",
    "        live.log_metric(f\"train/loss\", train_epoch_loss)\n",
    "        live.log_metric(f\"train/acc\", train_acc)\n",
    "        live.log_metric(f\"train/f1\", train_f1)\n",
    "\n",
    "    \n",
    "        # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # evaluation step\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        running_loss_test= 0.0\n",
    "        tepoch = tqdm(test_loader, desc=f\"Test {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in tepoch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs) #feed forward\n",
    "            loss = criterion_test(outputs, labels) #calculate the loss\n",
    "            running_loss_test += loss.item() * inputs.size(0) #accumulate the loss\n",
    "\n",
    "            # metric.update() updates the metric state with new data\n",
    "            metric_acc.update(outputs, labels)\n",
    "            metric_f1.update(outputs, labels)\n",
    "            tepoch.set_postfix(test_loss=loss.item(), test_acc=metric_acc.compute().item(), test_f1=metric_f1.compute().item())\n",
    "\n",
    "        test_epoch_loss = running_loss_test / len(test_loader.dataset)\n",
    "        test_acc = metric_acc.compute().item()\n",
    "        test_f1 = metric_f1.compute().item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], test_loss: {test_epoch_loss:.4f},  test_acc: {test_acc:.4f}, test_f1-Macro: {test_f1:.4f}\")\n",
    "          # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        \n",
    "        live.log_metric(f\"test/loss\", test_epoch_loss)\n",
    "        live.log_metric(f\"test/acc\", test_acc)\n",
    "        live.log_metric(f\"test/f1\", test_f1)\n",
    "        \n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # Guardar el modelo si la pérdida es la mejor hasta ahora\n",
    "        if (test_f1 - best_f1) > min_delta:\n",
    "            print(\"Improved test F1 from {:.4f} to {:.4f}. Saving model...\".format(best_f1, test_f1))\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_epoch_loss,\n",
    "                'test_loss': test_epoch_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'test_f1': test_f1,\n",
    "                'train_f1': train_f1,\n",
    "            }, save_path)\n",
    "            best_acc = test_acc\n",
    "            best_f1 = test_f1\n",
    "            patience = early_stopping_patience\n",
    "            \n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "        live.next_step() #finish epoch\n",
    "    print('Finished Training')\n",
    "    dict_model = torch.load(save_path)\n",
    "    dict_results = {'epoch':dict_model['epoch'],'train_loss': dict_model['train_loss'], 'test_loss':dict_model['test_loss'], 'train_acc':dict_model['train_acc'],\n",
    "                    'test_acc':dict_model['test_acc'], 'test_f1':dict_model['test_f1'], 'train_f1':dict_model['train_f1']}\n",
    "   # model = model.load_state_dict(dict_model['model_state_dict'])\n",
    "    return  dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (17472, 7), Testing shape: (4369, 7)\n"
     ]
    }
   ],
   "source": [
    "df_audio_train = pd.read_csv(TRAIN_CSV)\n",
    "df_audio_test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f'Training shape: {df_audio_train.shape}, Testing shape: {df_audio_test.shape}')\n",
    "\n",
    "save_path = f'PicklesAndFinalModel/{ESPEC}_preprocess_data_3_chans_history_torch_baseline_convnext_femto_{IMG_SIZE}_custom_exp_{exp_num}.pth'\n",
    "save_dict_results = f'PicklesAndFinalModel/{ESPEC}_preprocess_data_3_chans_history_torch_baseline_convnext_femto_{IMG_SIZE}_custom_exp_{exp_num}.pkl'\n",
    "\n",
    "\n",
    "skf= StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "history_list = []\n",
    "val_acc = []\n",
    "val_f1 = []\n",
    "\n",
    "test_acc = []\n",
    "test_f1 = []\n",
    "\n",
    "mixup = v2.MixUp(alpha=0.2, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    return mixup(*default_collate(batch))\n",
    "\n",
    "\n",
    "def cross_validation():\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(df_audio_train, df_audio_train['forgery_type_index'])):\n",
    "                \n",
    "        # Crear DataFrames para conjuntos de entrenamiento y validación\n",
    "        df_train_audios = df_audio_train.iloc[train_index]\n",
    "        df_val_audios = df_audio_train.iloc[val_index]\n",
    "        \n",
    "        if ESPEC == 'mfcc':\n",
    "            if AUG:\n",
    "                print('MFCC ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc_aug)\n",
    "                ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_mfcc)\n",
    "            else:\n",
    "                print('MFCC ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc)\n",
    "                ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_mfcc)\n",
    "        elif ESPEC == 'mel':\n",
    "            if AUG:\n",
    "                print('MEL ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data_train_aug)\n",
    "                ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_data)\n",
    "            else:\n",
    "                print('MEL ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data)\n",
    "                ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_data)\n",
    "\n",
    "        \n",
    "        dataloader_train = DataLoader(ds_train_audios, batch_size=batch_size, shuffle=True, num_workers=16, collate_fn=collate_fn)\n",
    "        dataloader_val = DataLoader(ds_val_audios, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "        \n",
    "        \n",
    "        # Crear una instancia del nuevo modelo\n",
    "        model = CustomConvNeXt(N=N)\n",
    "        # model = NewCustomCNN()\n",
    "\n",
    "        # Obtener el número total de parámetros entrenables\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'Params: {params}')\n",
    "        print(f\"Num. of trainable parameters: {total_params}\")\n",
    "\n",
    "        # Definir la función de pérdida y el optimizador\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "        #optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay, momentum=0.9)\n",
    "\n",
    "        if scheduler_learning:\n",
    "            #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min= 1E-4)\n",
    "            scheduler = ExponentialLR(optimizer, gamma=GAMMA_TO_DECAY)\n",
    "            #scheduler = None#CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=0.000001)\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        model.to(device)\n",
    "        label_counts = df_train_audios['forgery_type_index'].value_counts()\n",
    "        label_counts = label_counts.reindex(label_counts.index[::-1])\n",
    "        # Cambiar las etiquetas de 0, 1 y 2\n",
    "        label_counts.index = ['Original_Audio' if label == 0 else 'Copy_Move_Audio' if label == 1 else 'Splicing_Audio' for label in label_counts.index]\n",
    "        class_counts = [label_counts['Original_Audio'], label_counts['Copy_Move_Audio'], label_counts['Splicing_Audio']]  # Lista con el número de muestras de cada clase\n",
    "        total_samples = sum(label_counts)\n",
    "        weights = [total_samples / count for count in label_counts]\n",
    "\n",
    "        # Multiplica los pesos por el factor de aumento\n",
    "        weights_list = [weight * f_weighted for weight in weights]\n",
    "        print(weights_list)\n",
    "        \n",
    "        if WEIGHT:\n",
    "            print('WEIGHTED LOSS')\n",
    "            weight_classes = torch.tensor(weights_list).float()\n",
    "            weight_classes = weight_classes.to(device)\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weight_classes, label_smoothing=label_smoothing)\n",
    "            criterion_val = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "            criterion_val = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        history = train_model(fold, model, dataloader_train, dataloader_val, criterion, criterion_val,optimizer, scheduler, num_epochs=num_epochs, \n",
    "                            early_stopping_patience=early_stopping_patience,\n",
    "                            save_path=save_path)\n",
    "\n",
    "        history_list.append(history)\n",
    "        value_val = history['val_acc']\n",
    "        value_val_f1 = history['val_f1']\n",
    "        print(f'Fold {fold} finished, Val_accuracy: {value_val}, Val_f1: {value_val_f1}')\n",
    "        val_acc.append(history['val_acc'])\n",
    "        val_f1.append(history['val_f1'])\n",
    "\n",
    "        # Guarda history_list en un archivo con Pickle\n",
    "        print('Mean val_acc:', np.mean(val_acc), 'Mean val_f1:', np.mean(val_f1))\n",
    "        with open(save_dict_results, 'wb') as f:\n",
    "                pickle.dump(history_list, f)        \n",
    "                \n",
    "        print(history_list)    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    with Live(report=\"notebook\", exp_name=exp_name) as live:\n",
    "        df_audio_train = pd.read_csv(TRAIN_CSV)\n",
    "        df_audio_test = pd.read_csv(TEST_CSV)\n",
    "        live.log_params(params)\n",
    "        \n",
    "        \n",
    "        # Obtener nombres de videos para los conjuntos de entrenamiento y validación\n",
    "        train_video_names = df_audio_train['audio_name'].tolist()\n",
    "        val_video_names = df_audio_test['audio_name'].tolist()\n",
    "        \n",
    "        # Filtrar df_audio para obtener solo las filas correspondientes a los conjuntos de entrenamiento y validación\n",
    "        df_train_audios = df_audio_train[df_audio_train['audio_name'].str.contains('|'.join(train_video_names))]\n",
    "        df_test_audios = df_audio_test[df_audio_test['audio_name'].str.contains('|'.join(val_video_names))]\n",
    "        if ESPEC == 'mfcc':\n",
    "            if AUG:\n",
    "                print('MFCC ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc_aug)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mfcc)\n",
    "            else:\n",
    "                print('MFCC ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mfcc)\n",
    "        elif ESPEC == 'mel':\n",
    "            if AUG:\n",
    "                print('MEL ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data_train_aug)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_data)\n",
    "            else:\n",
    "                print('MEL ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_data)\n",
    "        # else:\n",
    "        #     if AUG:\n",
    "        #         print('MEL/MFCC/LFCC ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "        #         ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mel_mfcc_lfcc_aug)\n",
    "        #         ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mel_mfcc_lfcc)\n",
    "        #     else:\n",
    "        #         print('MEL/MFCC/LFCC ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "        #         ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mel_mfcc_lfcc)\n",
    "        #         ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mel_mfcc_lfcc)\n",
    "                \n",
    "                \n",
    "        dataloader_train = DataLoader(ds_train_audios, batch_size=batch_size, shuffle=True, num_workers=16, collate_fn=collate_fn)\n",
    "        dataloader_val = DataLoader(ds_test_audios, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "        \n",
    "        \n",
    "        # Crear una instancia del nuevo modelo\n",
    "        model = CustomConvNeXt(N=N)\n",
    "        # model = NewCustomCNN()\n",
    "        # Obtener el número total de parámetros entrenables\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"Num. of trainable parameters: {total_params}\")\n",
    "        print(params)\n",
    "        # Definir la función de pérdida y el optimizador\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "        #optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay, momentum=0.9)\n",
    "        if scheduler_learning:\n",
    "            #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min= 1E-4)\n",
    "            scheduler = ExponentialLR(optimizer, gamma=GAMMA_TO_DECAY)\n",
    "            #scheduler = None#CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=0.000001)\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        model.to(device)\n",
    "        label_counts = df_train_audios['forgery_type_index'].value_counts()\n",
    "        label_counts = label_counts.reindex(label_counts.index[::-1])\n",
    "\n",
    "        # Cambiar las etiquetas de 0, 1 y 2\n",
    "        label_counts.index = ['Original' if label == 0 else 'Copymove' if label == 1 else 'Splicing' for label in label_counts.index]\n",
    "        class_counts = [label_counts['Original'], label_counts['Copymove'], label_counts['Splicing']]  # Lista con el número de muestras de cada clase\n",
    "        total_samples = sum(label_counts)\n",
    "        weights = [total_samples / count for count in label_counts]\n",
    "\n",
    "\n",
    "        # Multiplica los pesos por el factor de aumento\n",
    "        weights_list = [weight * f_weighted for weight in weights]\n",
    "        print(weights_list)\n",
    "        if WEIGHT:\n",
    "            print('WEIGHTED LOSS')\n",
    "            weight_classes = torch.tensor(weights_list).float()\n",
    "            weight_classes = weight_classes.to(device)\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weight_classes, label_smoothing=label_smoothing)\n",
    "            criterion_test = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "            criterion_test = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        history = train_model_for_testing(live, model, dataloader_train, dataloader_val, criterion, criterion_test,optimizer, scheduler, num_epochs=num_epochs, \n",
    "                            early_stopping_patience=early_stopping_patience,\n",
    "                            save_path=save_path)\n",
    "\n",
    "        history_list.append(history)\n",
    "        value_test = history['test_acc']\n",
    "        value_test_f1 = history['test_f1']\n",
    "        print(f'Finished, test_f1: {value_test_f1}, test_f1: {value_test_f1}')\n",
    "        test_acc.append(history['test_acc'])\n",
    "        test_f1.append(history['test_f1'])\n",
    "        live.log_metric(f\"test/mean_test_acc\", np.mean(test_acc))\n",
    "        live.log_metric(f\"test/mean_test_f1\", np.mean(test_f1))\n",
    "        # Guarda history_list en un archivo con Pickle\n",
    "        print('Mean val_acc:', np.mean(test_acc), 'Mean test_f1:', np.mean(test_f1))\n",
    "        with open(save_dict_results, 'wb') as f:\n",
    "                pickle.dump(history_list, f)        \n",
    "                \n",
    "        print(history_list)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEL ESPECTOGRAM EXPERIMENT AUGMENTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'epochs': 50, 'lr': 0.0001, 'weight_decay': 0.01, 'batch_size': 32, 'early_stopping_patience': 10, 'label_smoothing': 0.1, 'num_classes': 3, 'n_fft': 1024, 'hop_length': 512, 'n_mels': 224, 'max_duration': 60, 'exp_name': 'CNNCustom_exp_43_mel_mix_up_cv_60s_mel_N_FFT_1024_MODEL_convnextv2_femto.fcmae_ft_in1k_AUG_True_WEIGHT_True_NMELS_224', 'exp_num': '43_mel_mix_up_cv', 'n_layers': 30, 'augmented': True, 'hidden_units': [256, 128, 64], 'audios_max_duration': 60, 'espectogram': 'mel', 'Freq_Mask': 5, 'Time_Mask': 10, 'scheduler': True, 'dropout_rate': 0.5, 'model_name': 'convnextv2_femto.fcmae_ft_in1k', 'weighted': True, 'img_size': 224, 'f_weighted': 1.5}\n",
      "Num. of trainable parameters: 2995651\n",
      "[19.4125, 3.2697286338116034, 3.232922127987664]\n",
      "WEIGHTED LOSS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/50:   0%|          | 0/437 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#testing()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 81\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(label_smoothing\u001b[38;5;241m=\u001b[39mlabel_smoothing)\n\u001b[1;32m     79\u001b[0m     criterion_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 81\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m history_list\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[1;32m     86\u001b[0m value_val \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[54], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(fold, model, train_loader, val_loader, criterion, criterion_val, optimizer, scheduler, num_epochs, save_path, early_stopping_patience, min_delta, val)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#for epoch in range(num_epochs):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     train_epoch_loss, train_acc,train_f1 \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,  Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1-Macro: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# metric.reset() reset metric states. It's typically called after the epoch completes.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[53], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, epoch, train_loader, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#feed forward\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m#calculate the loss\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\u001b[38;5;66;03m#backward pass to calculate the gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 50\u001b[0m, in \u001b[0;36mCustomConvNeXt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#x = self.avgpool(x)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#x = torch.flatten(x, 1)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#x = self.fc(x)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_layer(x)\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/timm/models/convnext.py:420\u001b[0m, in \u001b[0;36mConvNeXt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 420\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/timm/models/convnext.py:411\u001b[0m, in \u001b[0;36mConvNeXt.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 411\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages(x)\n\u001b[1;32m    413\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/TFG/Model/tfg/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#testing()\n",
    "cross_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
