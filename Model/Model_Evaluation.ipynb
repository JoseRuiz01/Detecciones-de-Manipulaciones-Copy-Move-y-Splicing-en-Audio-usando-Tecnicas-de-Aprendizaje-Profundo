{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "**Autores**: \n",
    "\n",
    "José Antonio Ruiz Heredia (josrui05@ucm.es) \n",
    "\n",
    "Néstor Marín \n",
    "\n",
    "**Descripción**: \n",
    "\n",
    "En este código realizamos la evaluación del modelo final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias y las rutas de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'schedulefree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mschedulefree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adamw_schedulefree, sgd_schedulefree\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'schedulefree'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR,CosineAnnealingWarmRestarts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import random\n",
    "from schedulefree import adamw_schedulefree, sgd_schedulefree\n",
    "import pickle\n",
    "import torch\n",
    "from dvclive import Live\n",
    "\n",
    "from torchaudio_augmentations import (\n",
    "    Compose,\n",
    "    LowPassFilter,\n",
    "    HighLowPass,\n",
    "    Noise,\n",
    "    PitchShift,\n",
    "    RandomApply,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import timm\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Suprimir todas las advertencias\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Suprimir una advertencia específica por su categoría\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# # Suprimir una advertencia específica por su mensaje\n",
    "# warnings.filterwarnings(\"ignore\", message=\"Some classes do not exist in the target\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#######################################################\n",
    "#Audio hyper-param\n",
    "# ESPEC = 'mfcc'\n",
    "ESPEC = 'mel'\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 8# 20\n",
    "N_FFT = 1024\n",
    "PRETRAINED = True\n",
    "N = 30\n",
    "if MAX_DURATION == 20:\n",
    "    if N_FFT == 2048:\n",
    "        TIME_MASK_PARAM =10\n",
    "        FREQ_MASK_PARAM = 5\n",
    "        HOP_LENGTH = 1024\n",
    "    else:\n",
    "        TIME_MASK_PARAM = 10\n",
    "        FREQ_MASK_PARAM = 5\n",
    "        HOP_LENGTH = 512\n",
    "else:\n",
    "    if N_FFT == 2048:\n",
    "        TIME_MASK_PARAM = 20\n",
    "        FREQ_MASK_PARAM = 5\n",
    "        HOP_LENGTH = 1024\n",
    "    else:\n",
    "        TIME_MASK_PARAM = 40\n",
    "        FREQ_MASK_PARAM = 5\n",
    "        HOP_LENGTH = 512\n",
    "        \n",
    "N_MELS = 224 #128\n",
    "N_MFCC = N_MELS\n",
    "N_LFCC = N_MELS\n",
    "RESIZE = False\n",
    "IMG_SIZE = 224 #384, #300 , 240\n",
    "NUM_CLASSES = 3\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "num_samples = MAX_DURATION * SAMPLE_RATE\n",
    "\n",
    "########################################################\n",
    "#Training hyper-param\n",
    "batch_size = 32\n",
    "learning_rate = 1E-4\n",
    "scheduler_learning = True\n",
    "weight_decay = 1E-3\n",
    "\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 10\n",
    "label_smoothing = 0.1\n",
    "STEP_TO_DECAY = 1\n",
    "GAMMA_TO_DECAY = 0.95\n",
    "WEIGHT = True\n",
    "f_weighted = 1.5\n",
    "AUG= False\n",
    "\n",
    "HIDDEN_UNITS=[256,128, 64]\n",
    "DROPOUT_RATE = 0.5\n",
    "num_samples = MAX_DURATION * SAMPLE_RATE\n",
    "MAX_PADDING = 118989\n",
    "\n",
    "\n",
    "# exp_name = f\"from_scratch_exp_{exp_num}\"\n",
    "params = {'epochs': num_epochs, 'lr': learning_rate, 'weight_decay': weight_decay, \n",
    "          'batch_size': batch_size, 'early_stopping_patience': early_stopping_patience,\n",
    "          'label_smoothing': label_smoothing, 'num_classes': NUM_CLASSES, 'n_fft': N_FFT,\n",
    "          'hop_length': HOP_LENGTH,'n_mels': N_MELS, 'max_duration': MAX_DURATION, \n",
    "          'exp_name': exp_name, 'exp_num': exp_num, 'n_layers': N, 'augmented': AUG, 'hidden_units': HIDDEN_UNITS, \n",
    "          'audios_max_duration': MAX_DURATION, 'espectogram':ESPEC, 'Freq_Mask': FREQ_MASK_PARAM, \n",
    "          'Time_Mask': TIME_MASK_PARAM, 'scheduler': scheduler_learning, 'dropout_rate': DROPOUT_RATE, 'model_name': MODEL_NAME\n",
    "          , 'weighted': WEIGHT, 'img_size': IMG_SIZE, 'f_weighted': f_weighted}\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.audio_df = annotations_file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = torchaudio.load(self.audio_df.iloc[idx, 0])\n",
    "        label = self.audio_df.iloc[idx, 4]\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return audio, label\n",
    "\n",
    "\n",
    "def preprocess_data(audio):\n",
    "     # Load the audio file\n",
    "  \n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    padding_needed = (MAX_PADDING) - WAVEFORM.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "   \n",
    "    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n",
    "\n",
    "    n_fft = N_FFT\n",
    "    win_length = None\n",
    "    hop_length = HOP_LENGTH\n",
    "    n_mels = N_MELS\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(waveform_padding)\n",
    "    \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n",
    "    else:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())  \n",
    "    log_mel_spectrogram_norm =log_mel_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    \n",
    "    if  RESIZE: \n",
    "        resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "        log_mel_spectrogram_norm_rgb = resize_image(log_mel_spectrogram_norm_rgb)/255\n",
    "    else:\n",
    "        log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb/255\n",
    "    log_mel_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    log_mel_imagenet = log_mel_normalize(log_mel_spectrogram_norm_rgb)\n",
    "    return log_mel_imagenet\n",
    "\n",
    "\n",
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, dropout_rate):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # Agregar la primera capa oculta con la entrada original\n",
    "        self.layers.append(torch.nn.Linear(input_size, hidden_units[0]))\n",
    "        self.layers.append(torch.nn.BatchNorm1d(hidden_units[0]))\n",
    "        self.layers.append(torch.nn.ReLU())\n",
    "        self.layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Agregar el resto de las capas ocultas\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            self.layers.append(torch.nn.Linear(hidden_units[i], hidden_units[i+1]))\n",
    "            self.layers.append(torch.nn.BatchNorm1d(hidden_units[i+1]))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        self.layers.append(torch.nn.Linear(hidden_units[-1], NUM_CLASSES))\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CustomConvNeXt(nn.Module):\n",
    "    def __init__(self, N=0):\n",
    "        super(CustomConvNeXt, self).__init__()\n",
    "        \n",
    "        # Cargar el modelo preentrenado\n",
    "        self.pretrained_model  = timm.create_model(MODEL_NAME, pretrained=PRETRAINED, num_classes=0, global_pool='avg') #convnextv2_femto.fcmae_ft_in1k.fcmae_ft_in22k_in1k_384 \n",
    "        #timm.create_model('convnextv2_nano.fcmae_ft_in22k_in1k', pretrained=True, num_classes=0)\n",
    "        self.n_layers = N\n",
    "        # Congelar todas las capas primero\n",
    "        for name, param in self.pretrained_model.named_parameters():\n",
    "           param.requires_grad = False\n",
    "\n",
    "        # Descongelar las últimas 20 capas que no son BatchNormalization\n",
    "        unfrozen_count = 0\n",
    "        for name, param in reversed(list(self.pretrained_model.named_parameters())):\n",
    "            if 'bn' not in name and unfrozen_count < self.n_layers:\n",
    "                param.requires_grad = True\n",
    "                unfrozen_count += 1\n",
    "        \n",
    "        \n",
    "        #self.add_vit = self.pretrained_model.num_features\n",
    "        self.additional_layer = MLPModel(self.pretrained_model.num_features, hidden_units=HIDDEN_UNITS, dropout_rate= DROPOUT_RATE)\n",
    "        #self.additional_layer = torch.nn.Linear(self.pretrained_model.num_features, NUM_CLASSES)\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        #x = self.avgpool(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\n",
    "        x = self.additional_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "# testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio_train = pd.read_parquet(f'/home/gass/audio-forgery-detection/data/train.parquet')\n",
    "df_audio_test = pd.read_parquet(f'/home/gass/audio-forgery-detection/data/test.parquet')\n",
    "\n",
    "print(f'Training shape: {df_audio_train.shape}, Testing shape: {df_audio_test.shape}')\n",
    "\n",
    "ds_val_audios =  CustomImageDataset(df_audio_test, transform=preprocess_data)\n",
    "dataloader_test = DataLoader(ds_val_audios, batch_size=batch_size, shuffle=False, num_workers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_pickle = 'PicklesAndFinalModel/mel_preprocess_data_3_chans_history_torch_baseline_convnext_femto_224_custom_exp_2_mel_cv_aug_sin_resize.pkl'\n",
    "\n",
    "\n",
    "# Cargar el archivo pickle\n",
    "with open(archivo_pickle, 'rb') as archivo:\n",
    "    datos = pickle.load(archivo)\n",
    "\n",
    "# Utilizar los datos cargados\n",
    "print(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'PicklesAndFinalModel/test_mel_preprocess_data_3_chans_history_torch_baseline_convnext_femto_224_custom_exp_8_mel_test_sin_resize_mel_best_model.pth'\n",
    "# Cargar el estado del modelo\n",
    "modelo_estado = torch.load(model_path)\n",
    "# Crear una instancia del modelo\n",
    "model = CustomConvNeXt()  # Reemplaza \"TuModelo\" con el nombre de tu modelo\n",
    "\n",
    "# Cargar el estado en el modelo\n",
    "model.load_state_dict(modelo_estado['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval() # Set the model to evaluation mode\n",
    "# # Hacer predicciones con el modelo\n",
    "predict_list = []\n",
    "predict_list_max = []\n",
    "predicted_TH = []\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader_test:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Make prediction\n",
    "        outputs = model(inputs) #feed forward\n",
    "        predict_list.extend(F.softmax(outputs, dim=1).cpu().numpy()) # apply softmax\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predict_list_max.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_list_max_TH= []\n",
    "# predicted = [1 if elements[1] > 0.5 else 0 for elements in predict_list]\n",
    "# predict_list_max_TH.extend(predicted)\n",
    "\n",
    "# Suponiendo que predict_list_max son tus predicciones y df_test_audios.label son las etiquetas verdaderas\n",
    "predictions = predict_list_max#df_test_audios.predictions\n",
    "true_labels = df_audio_test.label\n",
    "\n",
    "# Genera el informe de clasificación\n",
    "report = classification_report(true_labels, predictions, digits=3)\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Genera la matriz de confusión\n",
    "conf_mat = confusion_matrix(true_labels, predictions)\n",
    "print(conf_mat)\n",
    "# Importar la paleta de colores \"Blues\"\n",
    "cmap = sns.color_palette(\"Blues\")\n",
    "\n",
    "# Definir etiquetas personalizadas para los ejes x e y\n",
    "class_labels = [\"Real\", \"Copy-move\", \"Splicing\"]\n",
    "conf_mat_percent = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis] * 100\n",
    "# Visualizar la matriz de confusión en porcentajes con la paleta de colores \"Blues\"\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat_percent, annot=True, fmt='.2f', cmap=cmap, annot_kws={\"size\": 14})\n",
    "plt.xlabel('Predicción', fontsize=16)\n",
    "plt.ylabel('Etiqueta real', fontsize=16)\n",
    "plt.xticks(ticks=[0.5, 1.5, 2.5], labels=class_labels, fontsize=12)\n",
    "plt.yticks(ticks=[0.5, 1.5, 2.5], labels=class_labels, fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
