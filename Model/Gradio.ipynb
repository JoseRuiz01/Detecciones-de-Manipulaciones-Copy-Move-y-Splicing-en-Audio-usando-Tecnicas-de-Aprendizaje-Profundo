{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio\n",
    "\n",
    "**Autores**: \n",
    "\n",
    "José Antonio Ruiz Heredia (josrui05@ucm.es) \n",
    "\n",
    "Néstor Marín \n",
    "\n",
    "**Descripción**: \n",
    "\n",
    "En este código generamos una interfaz UI para probar el modelo con nuevas muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antes de ejecutar este archivo, asegurarse de haber instalado los prerrequisitos ejecutando el script \"Prerrequisitos.py\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torchaudio.transforms import Resample\n",
    "import gradio as gr\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESPEC = 'mel'\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 8\n",
    "N_FFT = 1024\n",
    "PRETRAINED = True\n",
    "N = 30\n",
    "TIME_MASK_PARAM = 10\n",
    "FREQ_MASK_PARAM = 5\n",
    "HOP_LENGTH = 512\n",
    "        \n",
    "N_MELS = 224 \n",
    "N_MFCC = N_MELS\n",
    "N_LFCC = N_MELS\n",
    "RESIZE = False\n",
    "IMG_SIZE = 224 \n",
    "NUM_CLASSES = 3\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "num_samples = MAX_DURATION * SAMPLE_RATE\n",
    "MAX_PADDING = 118989\n",
    "\n",
    "num_samples = MAX_DURATION * SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvNextV2ForImageClassification were not initialized from the model checkpoint at facebook/convnextv2-femto-1k-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([3, 384]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7893\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7893/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para añadir el preprocesado\n",
    "def preprocess_audio(audio):\n",
    "    WAVEFORM, SAMPLE_RATE = torchaudio.load(audio.name)\n",
    "\n",
    "    if SAMPLE_RATE != 16000:\n",
    "        resampler = Resample(orig_freq=SAMPLE_RATE, new_freq=16000)\n",
    "        WAVEFORM = resampler(WAVEFORM)\n",
    "\n",
    "    padding_needed = (MAX_PADDING) - WAVEFORM.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n",
    "\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=N_FFT,\n",
    "        win_length=None,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=N_MELS,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    melspec = mel_spectrogram(waveform_padding)\n",
    "\n",
    "    # Aplicar conversión a log mel\n",
    "    log_mel_spectrogram = torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "\n",
    "    # Normalizar el log mel espectrograma\n",
    "    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n",
    "    else:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())  \n",
    "    \n",
    "    log_mel_spectrogram_norm = log_mel_spectrogram_norm[0] * 255\n",
    "\n",
    "    # Convertir el log mel espectrograma a imagen\n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1) \n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb / 255\n",
    "\n",
    "\n",
    "    #Convertir el espectrograma de mel a imagen\n",
    "    # melspec = melspec.unsqueeze(0)\n",
    "    # melspec = torch.nn.functional.interpolate(melspec, size=(224, 224))\n",
    "    # melspec = melspec.repeat(1, 3, 1, 1)\n",
    "    # melspec = melspec.squeeze().permute(1, 2, 0).numpy()\n",
    "    # melspec = Image.fromarray((melspec * 255).astype(np.uint8))\n",
    "\n",
    "    # Normalizar la imagen\n",
    "    # melspec = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    return log_mel_spectrogram_norm_rgb\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Función para predecir la clase del audio\n",
    "def classify_audio(audio_files):\n",
    "    results = []  \n",
    "    processor = AutoImageProcessor.from_pretrained('facebook/convnextv2-femto-1k-224')\n",
    "    for audio_file in audio_files:\n",
    "        image = preprocess_audio(audio_file)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['pixel_values'])\n",
    "        probabilities = torch.softmax(output, dim=1).squeeze()\n",
    "        class_idx = torch.argmax(probabilities).item()\n",
    "        classes = [\"original\", \"splicing\", \"copy-move\"]\n",
    "        results.append(classes[class_idx])\n",
    "    return results\n",
    "\n",
    "\n",
    "class AudioClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.model = AutoModelForImageClassification.from_pretrained(\n",
    "            'facebook/convnextv2-femto-1k-224', num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = AudioClassifier()\n",
    "\n",
    "# Cargar el modelo\n",
    "state_dict = torch.load('PicklesAndFinalModel/test_mel_preprocess_data_3_chans_history_torch_baseline_convnext_femto_224_custom_exp_8_mel_test_sin_resize_mel_best_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# Filtrar el state_dict para eliminar claves no necesarias\n",
    "filtered_state_dict = {k: v for k, v in state_dict['model_state_dict'].items() if k in model.state_dict().keys()}\n",
    "\n",
    "# Cargar el state_dict filtrado\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Interfaz de Gradio\n",
    "with gr.Blocks(gr.themes.Default(), title=\"Voice Cloning Demo\") as demo:\n",
    "    gr.Markdown(\"Audio Classifier\")\n",
    "    with gr.Tab(\"Inference\"):\n",
    "        with gr.Column() as col1:\n",
    "            upload_file = gr.File(\n",
    "                file_count=\"multiple\",\n",
    "                label=\"Select here the audio files\",\n",
    "            )\n",
    "\n",
    "            label = gr.Textbox(label=\"Predicted Class(es)\")\n",
    "\n",
    "            button = gr.Button(\"Classify\")\n",
    "\n",
    "            button.click(fn=classify_audio, inputs=upload_file, outputs=label)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
