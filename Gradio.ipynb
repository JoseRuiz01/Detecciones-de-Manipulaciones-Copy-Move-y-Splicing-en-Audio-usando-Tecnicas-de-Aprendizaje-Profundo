{"cells":[{"cell_type":"markdown","metadata":{"id":"Tkgjj7Pv5TzP"},"source":["# Gradio\n","\n","**Autores**:\n","\n","José Antonio Ruiz Heredia (josrui05@ucm.es)\n","\n","Néstor Marín\n","\n","**Descripción**:\n","\n","En este código generamos una interfaz UI para probar el modelo con nuevas muestras."]},{"cell_type":"markdown","metadata":{"id":"68h-2rXU5Tzb"},"source":["**Antes de ejecutar este archivo, asegurarse de haber instalado los prerrequisitos ejecutando el script \"Prerrequisitos.py\"**"]},{"cell_type":"markdown","metadata":{"id":"PxjryRNW5Tzb"},"source":["Librerias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmD3c7ct5Tzb"},"outputs":[],"source":["import torch\n","import torchaudio\n","import torchaudio.transforms as T\n","from torchaudio.transforms import Resample\n","import gradio as gr\n","import torchvision.transforms as transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import numpy as np\n","from transformers import AutoImageProcessor, AutoModelForImageClassification\n","from PIL import Image\n","import torch.nn as nn\n","import timm"]},{"cell_type":"markdown","metadata":{"id":"6ABBRwiN5Tzc"},"source":["Parámetros"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jE6TOJfU5Tzc"},"outputs":[],"source":["ESPEC = 'mel'\n","SAMPLE_RATE = 16000\n","MAX_DURATION = 8\n","N_FFT = 1024\n","PRETRAINED = True\n","N = 30\n","TIME_MASK_PARAM = 10\n","FREQ_MASK_PARAM = 5\n","HOP_LENGTH = 512\n","\n","N_MELS = 224\n","N_MFCC = N_MELS\n","N_LFCC = N_MELS\n","RESIZE = False\n","IMG_SIZE = 224\n","NUM_CLASSES = 3\n","SAMPLE_RATE = 16000\n","\n","num_samples = MAX_DURATION * SAMPLE_RATE\n","MAX_PADDING = 118989\n","HIDDEN_UNITS=[256,128, 64]\n","num_samples = MAX_DURATION * SAMPLE_RATE\n","DROPOUT_RATE = 0.5"]},{"cell_type":"markdown","metadata":{"id":"ESj9AGFi5Tzc"},"source":["Clasificación"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmL2EU6k5Tzd"},"outputs":[],"source":["# Función para añadir el preprocesado\n","def preprocess_audio(audio):\n","    WAVEFORM, SAMPLE_RATE = torchaudio.load(audio.name)\n","\n","    if SAMPLE_RATE != 16000:\n","        resampler = Resample(orig_freq=SAMPLE_RATE, new_freq=16000)\n","        WAVEFORM = resampler(WAVEFORM)\n","\n","    padding_needed = (MAX_PADDING) - WAVEFORM.shape[1]\n","    padding = max(padding_needed, 0)\n","    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n","\n","    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","        sample_rate=SAMPLE_RATE,\n","        n_fft=N_FFT,\n","        win_length=None,\n","        hop_length=HOP_LENGTH,\n","        center=True,\n","        pad_mode=\"reflect\",\n","        power=2.0,\n","        norm=\"slaney\",\n","        n_mels=N_MELS,\n","        mel_scale=\"htk\",\n","    )\n","\n","    melspec = mel_spectrogram(waveform_padding)\n","\n","    # Aplicar conversión a log mel\n","    log_mel_spectrogram = torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n","\n","    # Normalizar el log mel espectrograma\n","    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n","        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n","    else:\n","        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())\n","\n","    log_mel_spectrogram_norm = log_mel_spectrogram_norm[0] * 255\n","\n","    # Convertir el log mel espectrograma a imagen\n","    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1)\n","    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb / 255\n","\n","\n","    #Convertir el espectrograma de mel a imagen\n","    # melspec = melspec.unsqueeze(0)\n","    # melspec = torch.nn.functional.interpolate(melspec, size=(224, 224))\n","    # melspec = melspec.repeat(1, 3, 1, 1)\n","    # melspec = melspec.squeeze().permute(1, 2, 0).numpy()\n","    # melspec = Image.fromarray((melspec * 255).astype(np.uint8))\n","\n","    # Normalizar la imagen\n","    # melspec = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","    log_mel_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n","    log_mel_imagenet = log_mel_normalize(log_mel_spectrogram_norm_rgb)\n","    return log_mel_imagenet\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kugOnBZQ5Tzd"},"outputs":[],"source":["\n","\n","class MLPModel(torch.nn.Module):\n","    def __init__(self, input_size, hidden_units, dropout_rate):\n","        super(MLPModel, self).__init__()\n","        self.hidden_units = hidden_units\n","        self.dropout_rate = dropout_rate\n","        self.layers = torch.nn.ModuleList()\n","\n","        # Agregar la primera capa oculta con la entrada original\n","        self.layers.append(torch.nn.Linear(input_size, hidden_units[0]))\n","        self.layers.append(torch.nn.BatchNorm1d(hidden_units[0]))\n","        self.layers.append(torch.nn.ReLU())\n","        self.layers.append(torch.nn.Dropout(dropout_rate))\n","\n","        # Agregar el resto de las capas ocultas\n","        for i in range(len(hidden_units) - 1):\n","            self.layers.append(torch.nn.Linear(hidden_units[i], hidden_units[i+1]))\n","            self.layers.append(torch.nn.BatchNorm1d(hidden_units[i+1]))\n","            self.layers.append(torch.nn.ReLU())\n","            self.layers.append(torch.nn.Dropout(dropout_rate))\n","        self.layers.append(torch.nn.Linear(hidden_units[-1], NUM_CLASSES))\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","class CustomConvNeXt(nn.Module):\n","    def __init__(self, N=0):\n","        super(CustomConvNeXt, self).__init__()\n","\n","        # Cargar el modelo preentrenado\n","        self.pretrained_model  = timm.create_model('convnextv2_femto.fcmae_ft_in1k', pretrained=PRETRAINED, num_classes=0, global_pool='avg') #convnextv2_femto.fcmae_ft_in1k.fcmae_ft_in22k_in1k_384\n","        #timm.create_model('convnextv2_nano.fcmae_ft_in22k_in1k', pretrained=True, num_classes=0)\n","        self.n_layers = N\n","        # Congelar todas las capas primero\n","        for name, param in self.pretrained_model.named_parameters():\n","           param.requires_grad = False\n","\n","        # Descongelar las últimas 20 capas que no son BatchNormalization\n","        unfrozen_count = 0\n","        for name, param in reversed(list(self.pretrained_model.named_parameters())):\n","            if 'bn' not in name and unfrozen_count < self.n_layers:\n","                param.requires_grad = True\n","                unfrozen_count += 1\n","\n","\n","        #self.add_vit = self.pretrained_model.num_features\n","        self.additional_layer = MLPModel(self.pretrained_model.num_features, hidden_units=HIDDEN_UNITS, dropout_rate= DROPOUT_RATE)\n","        #self.additional_layer = torch.nn.Linear(self.pretrained_model.num_features, NUM_CLASSES)\n","    def forward(self, x):\n","        x = self.pretrained_model(x)\n","        #x = self.avgpool(x)\n","        #x = torch.flatten(x, 1)\n","        #x = self.fc(x)\n","        x = self.additional_layer(x)\n","        return x\n","\n","\n","# Función para predecir la clase del audio\n","\n","# Inicializar el modelo\n","model = CustomConvNeXt()\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# Cargar el modelo\n","state_dict = torch.load('/home/gass/audio-forgery-detection/forgery_models/test_mel_preprocess_data_3_chans_history_torch_baseline_convnext_femto_224_custom_exp_8_mel_test_sin_resize_mel_best_model.pth',\n","                        map_location=torch.device('cuda'))\n","# Filtrar el state_dict para eliminar claves no necesarias\n","filtered_state_dict = {k: v for k, v in state_dict['model_state_dict'].items() if k in model.state_dict().keys()}\n","\n","# Cargar el state_dict filtrado\n","model.load_state_dict(state_dict['model_state_dict'])\n","model = model.to(device)\n","model.eval()\n","\n","def classify_audio(audio_files):\n","    results = []\n","    predict_list = []\n","    predict_list_max = []\n","    predicted_TH = []\n","    # processor = AutoImageProcessor.from_pretrained('facebook/convnextv2-femto-1k-224')\n","    with torch.no_grad():\n","        for audio_file in audio_files:\n","            image = preprocess_audio(audio_file)\n","            image = torch.unsqueeze(image, 0)\n","            # inputs = processor(images=image, return_tensors=\"pt\")\n","\n","            inputs = image.to(device)\n","            # Make prediction\n","            output = model(inputs) #feed forward\n","            # predict_list.extend(F.softmax(outputs, dim=1).cpu().numpy()) # apply softmax\n","            # _, predicted = torch.max(outputs, 1)\n","            # predict_list_max.extend(predicted.cpu().numpy())\n","            probabilities = torch.softmax(output, dim=1).squeeze()\n","            class_idx = torch.argmax(probabilities).item()\n","            classes = [\"original\",\"copy-move\", \"splicing\" ]\n","            results.append(classes[class_idx])\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tBIWcUg5Tzd","outputId":"4995bab8-5258-4da5-c9a5-d2ee478aeff0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7862\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":29,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n","torch.Size([1, 3, 224, 233])\n"]}],"source":["\n","\n","\n","\n","\n","# Interfaz de Gradio\n","with gr.Blocks(gr.themes.Default(), title=\"Voice Cloning Demo\") as demo:\n","    gr.Markdown(\"Audio Classifier\")\n","    with gr.Tab(\"Inference\"):\n","        with gr.Column() as col1:\n","            upload_file = gr.File(\n","                file_count=\"multiple\",\n","                label=\"Select here the audio files\",\n","            )\n","\n","            label = gr.Textbox(label=\"Predicted Class(es)\")\n","\n","            button = gr.Button(\"Classify\")\n","\n","            button.click(fn=classify_audio, inputs=upload_file, outputs=label)\n","\n","demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"tfg","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}