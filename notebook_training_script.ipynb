{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR,CosineAnnealingWarmRestarts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import random\n",
    "from torchaudio_augmentations import (\n",
    "    Compose,\n",
    "    LowPassFilter,\n",
    "    HighLowPass,\n",
    "    Noise,\n",
    "    PitchShift,\n",
    "    RandomApply,\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import timm\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#######################################################\n",
    "#Audio hyper-param\n",
    "# ESPEC = 'mfcc'\n",
    "ESPEC = 'mel'\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 60 # 20 seconds\n",
    "N_FFT = 1024\n",
    "PRETRAINED = True\n",
    "N = 30\n",
    "\n",
    "if N_FFT == 2048:\n",
    "    TIME_MASK_PARAM =10\n",
    "    FREQ_MASK_PARAM = 5\n",
    "    HOP_LENGTH = 1024\n",
    "else:\n",
    "    TIME_MASK_PARAM = 10\n",
    "    FREQ_MASK_PARAM = 5\n",
    "    HOP_LENGTH = 512\n",
    "\n",
    "        \n",
    "N_MELS = 224 #128\n",
    "N_MFCC = N_MELS\n",
    "N_LFCC = N_MELS\n",
    "RESIZE = False\n",
    "IMG_SIZE = 224 #384, #300 , 240\n",
    "NUM_CLASSES = 3\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "num_samples = MAX_DURATION * SAMPLE_RATE\n",
    "exp_num = '43_mel_mix_up_cv'\n",
    "\n",
    "MODEL_NAME = 'convnextv2_femto.fcmae_ft_in1k' # tf_efficientnetv2_b0.in1k,tf_efficientnetv2_b3.in21k_ft_in1k (240x240),tf_efficientnetv2_s.in21k_ft_in1k (300x300),\n",
    "#convnextv2_nano.fcmae_ft_in22k_in1k, convnextv2_femto.fcmae_ft_in1k, convnextv2_nano.fcmae_ft_in22k_in1k_384\n",
    "\n",
    "########################################################\n",
    "#Training hyper-param\n",
    "batch_size = 32\n",
    "learning_rate = 1E-4\n",
    "scheduler_learning = True\n",
    "weight_decay = 1E-2\n",
    "\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 10\n",
    "label_smoothing = 0.1\n",
    "STEP_TO_DECAY = 1\n",
    "GAMMA_TO_DECAY = 0.95\n",
    "WEIGHT = True\n",
    "f_weighted = 1.5\n",
    "AUG= True\n",
    "\n",
    "HIDDEN_UNITS=[256,128, 64]\n",
    "DROPOUT_RATE = 0.5\n",
    "SEED = 42\n",
    "SEED_AUG = 1234\n",
    "exp_name = f\"CNNCustom_exp_{exp_num}_{MAX_DURATION}s_{ESPEC}_N_FFT_{N_FFT}_MODEL_{MODEL_NAME}_AUG_{AUG}_WEIGHT_{WEIGHT}_NMELS_{N_MELS}\"\n",
    "# ########################################################\n",
    "torch.manual_seed(SEED_AUG)\n",
    "# Establecer la semilla para Numpy\n",
    "np.random.seed(SEED_AUG)\n",
    "# Establecer la semilla para Python random\n",
    "random.seed(SEED_AUG)\n",
    "aug_transforms = [\n",
    "                    RandomApply([Noise(min_snr=0.05, max_snr=0.1),],p=0.5,),\n",
    "                    RandomApply([PitchShift(sample_rate=SAMPLE_RATE, n_samples=num_samples, pitch_shift_max=3, pitch_shift_min=-3),],p=0.3,),\n",
    "                    #RandomApply([HighLowPass(sample_rate=SAMPLE_RATE) ],p=0.3,) \n",
    "                ]\n",
    "###################################################################\n",
    "metric_acc = MulticlassAccuracy(device=device)\n",
    "metric_f1 = MulticlassF1Score(device=device, num_classes=NUM_CLASSES, average='macro')\n",
    "\n",
    "\n",
    "# exp_name = f\"from_scratch_exp_{exp_num}\"\n",
    "params = {'epochs': num_epochs, 'lr': learning_rate, 'weight_decay': weight_decay, \n",
    "          'batch_size': batch_size, 'early_stopping_patience': early_stopping_patience,\n",
    "          'label_smoothing': label_smoothing, 'num_classes': NUM_CLASSES, 'n_fft': N_FFT,\n",
    "          'hop_length': HOP_LENGTH,'n_mels': N_MELS, 'max_duration': MAX_DURATION, \n",
    "          'exp_name': exp_name, 'exp_num': exp_num, 'n_layers': N, 'augmented': AUG, 'hidden_units': HIDDEN_UNITS, \n",
    "          'audios_max_duration': MAX_DURATION, 'espectogram':ESPEC, 'Freq_Mask': FREQ_MASK_PARAM, \n",
    "          'Time_Mask': TIME_MASK_PARAM, 'scheduler': scheduler_learning, 'dropout_rate': DROPOUT_RATE, 'model_name': MODEL_NAME\n",
    "          , 'weighted': WEIGHT, 'img_size': IMG_SIZE, 'f_weighted': f_weighted}\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.audio_df = annotations_file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = torchaudio.load(self.audio_df.iloc[idx, 0])\n",
    "        label = self.audio_df.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return audio, label\n",
    "\n",
    "\n",
    "def preprocess_mfcc_aug(audio):\n",
    "\n",
    "     # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    c_transform = Compose(aug_transforms)\n",
    "    transformed_audio = c_transform(WAVEFORM)\n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - transformed_audio.shape[1]\n",
    "    \n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(transformed_audio, (0, padding))\n",
    "\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        log_mels=False,\n",
    "        melkwargs={\n",
    "        'n_fft':N_FFT,\n",
    "        'win_length':None,\n",
    "        'hop_length':HOP_LENGTH,\n",
    "        'center':True,\n",
    "        'pad_mode':\"reflect\",\n",
    "        'power':2.0,\n",
    "        'norm':\"slaney\",\n",
    "        'n_mels':N_MELS,\n",
    "        'mel_scale':\"htk\" \n",
    "        })\n",
    "\n",
    "        \n",
    "    mfcc_spectrogram = mfcc_transform(waveform_padding)\n",
    "    \n",
    "    time_masking = T.TimeMasking(time_mask_param=TIME_MASK_PARAM)\n",
    "    freq_masking = T.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)\n",
    "\n",
    "    time_masked = time_masking(mfcc_spectrogram)\n",
    "    freq_masked = freq_masking(time_masked)\n",
    "    \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_mfcc_spectrogram= torchaudio.transforms.AmplitudeToDB()(freq_masked)\n",
    "    if log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min() != 0:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min()) / (log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min())\n",
    "    else:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min())  \n",
    "        \n",
    "    mfcc_spectrogram_norm =mfcc_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    mfcc_spectrogram_norm_rgb = mfcc_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "    mfcc_spectrogram_norm_rgb_resize = resize_image(mfcc_spectrogram_norm_rgb)/255\n",
    "    mfcc_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    mfcc_imagenet = mfcc_normalize(mfcc_spectrogram_norm_rgb_resize)\n",
    "    return mfcc_imagenet\n",
    "\n",
    "def preprocess_mfcc(audio):\n",
    "    # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - WAVEFORM.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n",
    "\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        log_mels=False,\n",
    "        melkwargs={\n",
    "        'n_fft':N_FFT,\n",
    "        'win_length':None,\n",
    "        'hop_length':HOP_LENGTH,\n",
    "        'center':True,\n",
    "        'pad_mode':\"reflect\",\n",
    "        'power':2.0,\n",
    "        'norm':\"slaney\",\n",
    "        'n_mels':N_MELS,\n",
    "        'mel_scale':\"htk\" \n",
    "        })\n",
    "\n",
    "        \n",
    "    mfcc_spectrogram = mfcc_transform(waveform_padding)\n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_mfcc_spectrogram= torchaudio.transforms.AmplitudeToDB()(mfcc_spectrogram)\n",
    "    if log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min() != 0:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min()) / (log_mel_mfcc_spectrogram.max() - log_mel_mfcc_spectrogram.min())\n",
    "    else:\n",
    "        mfcc_spectrogram_norm = (log_mel_mfcc_spectrogram - log_mel_mfcc_spectrogram.min())  \n",
    "        \n",
    "    mfcc_spectrogram_norm =mfcc_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    mfcc_spectrogram_norm_rgb = mfcc_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "    mfcc_spectrogram_norm_rgb_resize = resize_image(mfcc_spectrogram_norm_rgb)/255\n",
    "    mfcc_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    mfcc_imagenet = mfcc_normalize(mfcc_spectrogram_norm_rgb_resize)\n",
    "    return mfcc_imagenet\n",
    "    \n",
    "    \n",
    "\n",
    "def preprocess_data_train_aug(audio):\n",
    "    # Load the audio file\n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    \n",
    "    c_transform = Compose(aug_transforms)\n",
    "    transformed_audio = c_transform(WAVEFORM)\n",
    "    \n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - transformed_audio.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(transformed_audio, (0, padding))\n",
    "    \n",
    "    n_fft = N_FFT\n",
    "    win_length = None\n",
    "    hop_length = HOP_LENGTH\n",
    "    n_mels = N_MELS\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(waveform_padding)\n",
    "    #T.TimeStretch()\n",
    "\n",
    "    time_masking = T.TimeMasking(time_mask_param=TIME_MASK_PARAM)\n",
    "    freq_masking = T.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)\n",
    "    \n",
    "\n",
    "    time_masked = time_masking(melspec)\n",
    "    freq_masked = freq_masking(time_masked)\n",
    "        \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(freq_masked)\n",
    "    log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(freq_masked)\n",
    "    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n",
    "    else:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())  \n",
    "    log_mel_spectrogram_norm =log_mel_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    if  RESIZE: \n",
    "        resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "\n",
    "        log_mel_spectrogram_norm_rgb = resize_image(log_mel_spectrogram_norm_rgb)/255\n",
    "    else:\n",
    "        log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb/255\n",
    "    log_mel_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    log_mel_imagenet = log_mel_normalize(log_mel_spectrogram_norm_rgb)\n",
    "    return log_mel_imagenet\n",
    "\n",
    "def preprocess_data(audio):\n",
    "     # Load the audio file\n",
    "  \n",
    "    WAVEFORM, SAMPLE_RATE = audio\n",
    "    padding_needed = (SAMPLE_RATE * MAX_DURATION) - WAVEFORM.shape[1]\n",
    "    padding = max(padding_needed, 0)\n",
    "    waveform_padding = torch.nn.functional.pad(WAVEFORM, (0, padding))\n",
    "    \n",
    "    n_fft = N_FFT\n",
    "    win_length = None\n",
    "    hop_length = HOP_LENGTH\n",
    "    n_mels = N_MELS\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(waveform_padding)\n",
    "    \n",
    "    # log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    log_mel_spectrogram= torchaudio.transforms.AmplitudeToDB(top_db=80)(melspec)\n",
    "    if log_mel_spectrogram.max() - log_mel_spectrogram.min() != 0:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min()) / (log_mel_spectrogram.max() - log_mel_spectrogram.min())\n",
    "    else:\n",
    "        log_mel_spectrogram_norm = (log_mel_spectrogram - log_mel_spectrogram.min())  \n",
    "    log_mel_spectrogram_norm =log_mel_spectrogram_norm[0]*255\n",
    "    # log_mel_spectrogram_norm = torch.unsqueeze(log_mel_spectrogram_norm, 0)\n",
    "    log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm.repeat(3, 1, 1)  # Repite el canal en las dimensiones de los canales\n",
    "    resize_image = transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None)\n",
    "    if  RESIZE: \n",
    "        log_mel_spectrogram_norm_rgb = resize_image(log_mel_spectrogram_norm_rgb)/255\n",
    "    else:\n",
    "        log_mel_spectrogram_norm_rgb = log_mel_spectrogram_norm_rgb/255\n",
    "    log_mel_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    log_mel_imagenet = log_mel_normalize(log_mel_spectrogram_norm_rgb)\n",
    "    return log_mel_imagenet\n",
    "\n",
    "\n",
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, dropout_rate):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # Agregar la primera capa oculta con la entrada original\n",
    "        self.layers.append(torch.nn.Linear(input_size, hidden_units[0]))\n",
    "        self.layers.append(torch.nn.BatchNorm1d(hidden_units[0]))\n",
    "        self.layers.append(torch.nn.ReLU())\n",
    "        self.layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Agregar el resto de las capas ocultas\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            self.layers.append(torch.nn.Linear(hidden_units[i], hidden_units[i+1]))\n",
    "            self.layers.append(torch.nn.BatchNorm1d(hidden_units[i+1]))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        self.layers.append(torch.nn.Linear(hidden_units[-1], NUM_CLASSES))\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CustomConvNeXt(nn.Module):\n",
    "    def __init__(self, N=0):\n",
    "        super(CustomConvNeXt, self).__init__()\n",
    "        \n",
    "        # Cargar el modelo preentrenado\n",
    "        self.pretrained_model  = timm.create_model(MODEL_NAME, pretrained=PRETRAINED, num_classes=0, global_pool='avg') #convnextv2_femto.fcmae_ft_in1k.fcmae_ft_in22k_in1k_384 \n",
    "        #timm.create_model('convnextv2_nano.fcmae_ft_in22k_in1k', pretrained=True, num_classes=0)\n",
    "        self.n_layers = N\n",
    "        # Congelar todas las capas primero\n",
    "        for name, param in self.pretrained_model.named_parameters():\n",
    "           param.requires_grad = False\n",
    "\n",
    "        # Descongelar las últimas 20 capas que no son BatchNormalization\n",
    "        unfrozen_count = 0\n",
    "        for name, param in reversed(list(self.pretrained_model.named_parameters())):\n",
    "            if 'bn' not in name and unfrozen_count < self.n_layers:\n",
    "                param.requires_grad = True\n",
    "                unfrozen_count += 1\n",
    "        \n",
    "        \n",
    "        #self.add_vit = self.pretrained_model.num_features\n",
    "        self.additional_layer = MLPModel(self.pretrained_model.num_features, hidden_units=HIDDEN_UNITS, dropout_rate= DROPOUT_RATE)\n",
    "        #self.additional_layer = torch.nn.Linear(self.pretrained_model.num_features, NUM_CLASSES)\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        #x = self.avgpool(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\n",
    "        x = self.additional_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def convert_labels_to_binary(labels):\n",
    "    # Aplicar un umbral de 0.5\n",
    "    binary_labels = torch.argmax(labels, dim=1)\n",
    "    return binary_labels\n",
    "\n",
    "def train_epoch(model,epoch,  train_loader, criterion, optimizer, scheduler=None):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss_train = 0.0\n",
    "    # Iterate over the training data (forward pass and backward pass)\n",
    "    tepoch = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\")\n",
    "    for inputs, labels in tepoch:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #feed forward\n",
    "        loss = criterion(outputs, labels) #calculate the loss\n",
    "        loss.backward()#backward pass to calculate the gradients\n",
    "        optimizer.step()#update the weights\n",
    "        running_loss_train += loss.item() * inputs.size(0) #accumulate the loss\n",
    "        # metric.update() updates the metric state with new data\n",
    "        \n",
    "        binary_labels = convert_labels_to_binary(labels)\n",
    "        metric_acc.update(outputs, binary_labels)\n",
    "        metric_f1.update(outputs, binary_labels)\n",
    "        tepoch.set_postfix(loss=loss.item(), acc=metric_acc.compute().item(), f1=metric_f1.compute().item())\n",
    "    if scheduler and (epoch+1) % STEP_TO_DECAY == 0:\n",
    "        scheduler.step()\n",
    "    train_epoch_loss = running_loss_train / len(train_loader.dataset)\n",
    "    train_acc = metric_acc.compute().item()\n",
    "    train_f1 = metric_f1.compute().item()\n",
    "    return train_epoch_loss, train_acc, train_f1\n",
    "\n",
    "\n",
    "df_audio_train = pd.read_parquet('/home/gass/audio-sensitive-content-detection/data/dfTrainVideo.parquet')\n",
    "df_audio_test = pd.read_parquet('/home/gass/audio-sensitive-content-detection/data/dfTestVideo.parquet')\n",
    "print(f'Training shape: {df_audio_train.shape}, Testing shape: {df_audio_test.shape}')\n",
    "\n",
    "# Función para entrenamiento\n",
    "def train_model(fold, model, train_loader, val_loader, criterion, criterion_val,optimizer, scheduler=None, num_epochs=10, save_path=None, early_stopping_patience=3, min_delta = 0.001, val = 'val'):\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    patience = early_stopping_patience\n",
    "\n",
    "    #for epoch in range(num_epochs):\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        # Call the function\n",
    "        train_epoch_loss, train_acc,train_f1 =  train_epoch(model, epoch, train_loader, criterion, optimizer, scheduler)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}, Fold: {fold}, Loss: {train_epoch_loss:.4f},  Acc: {train_acc:.4f}, F1-Macro: {train_f1:.4f}\")\n",
    "\n",
    "        # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # evaluation step\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        running_loss_val= 0.0\n",
    "     \n",
    "        tepoch = tqdm(val_loader, desc=f\"Val {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in tepoch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs) #feed forward\n",
    "            loss = criterion_val(outputs, labels) #calculate the loss\n",
    "            running_loss_val += loss.item() * inputs.size(0) #accumulate the loss\n",
    "\n",
    "            # metric.update() updates the metric state with new data\n",
    "            metric_acc.update(outputs, labels)\n",
    "            metric_f1.update(outputs, labels)\n",
    "            tepoch.set_postfix(val_loss=loss.item(), val_acc=metric_acc.compute().item(), val_f1=metric_f1.compute().item())\n",
    "\n",
    "        val_epoch_loss = running_loss_val / len(val_loader.dataset)\n",
    "        val_acc = metric_acc.compute().item()\n",
    "        val_f1 = metric_f1.compute().item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Fold: {fold}, Val_loss: {val_epoch_loss:.4f},  Val_acc: {val_acc:.4f}, Val_f1-Macro: {val_f1:.4f}\")\n",
    "          # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        \n",
    "\n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # Guardar el modelo si la pérdida es la mejor hasta ahora\n",
    "        if (val_f1 - best_f1) > min_delta:\n",
    "            print(\"Improved val F1 from {:.4f} to {:.4f}. Saving model...\".format(best_f1, val_f1))\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_epoch_loss,\n",
    "                'val_loss': val_epoch_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'val_f1': val_f1,\n",
    "                'train_f1': train_f1,\n",
    "            }, save_path)\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "            patience = early_stopping_patience\n",
    "            \n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "    print('Finished Training')\n",
    "    dict_model = torch.load(save_path)\n",
    "    dict_results = {'epoch':dict_model['epoch'],'train_loss': dict_model['train_loss'], 'val_loss':dict_model['val_loss'], 'train_acc':dict_model['train_acc'],\n",
    "                    'val_acc':dict_model['val_acc'], 'val_f1':dict_model['val_f1'], 'train_f1':dict_model['train_f1']}\n",
    "   # model = model.load_state_dict(dict_model['model_state_dict'])\n",
    "    return  dict_results\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_model_for_testing( live, model, train_loader, test_loader, criterion, criterion_test,optimizer, scheduler=None, num_epochs=10, save_path=None, \n",
    "                early_stopping_patience=3, min_delta = 0.003, val = 'val'):\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    patience = early_stopping_patience\n",
    "\n",
    "    #for epoch in range(num_epochs):\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        # Call the function\n",
    "        train_epoch_loss, train_acc,train_f1 =  train_epoch(model, epoch, train_loader, criterion, optimizer, scheduler)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}, Loss: {train_epoch_loss:.4f},  Acc: {train_acc:.4f}, F1-Macro: {train_f1:.4f}\")\n",
    "\n",
    "        live.log_metric(f\"train/loss\", train_epoch_loss)\n",
    "        live.log_metric(f\"train/acc\", train_acc)\n",
    "        live.log_metric(f\"train/f1\", train_f1)\n",
    "\n",
    "    \n",
    "        # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # evaluation step\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        running_loss_test= 0.0\n",
    "        tepoch = tqdm(test_loader, desc=f\"Test {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in tepoch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs) #feed forward\n",
    "            loss = criterion_test(outputs, labels) #calculate the loss\n",
    "            running_loss_test += loss.item() * inputs.size(0) #accumulate the loss\n",
    "\n",
    "            # metric.update() updates the metric state with new data\n",
    "            metric_acc.update(outputs, labels)\n",
    "            metric_f1.update(outputs, labels)\n",
    "            tepoch.set_postfix(test_loss=loss.item(), test_acc=metric_acc.compute().item(), test_f1=metric_f1.compute().item())\n",
    "\n",
    "        test_epoch_loss = running_loss_test / len(test_loader.dataset)\n",
    "        test_acc = metric_acc.compute().item()\n",
    "        test_f1 = metric_f1.compute().item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], test_loss: {test_epoch_loss:.4f},  test_acc: {test_acc:.4f}, test_f1-Macro: {test_f1:.4f}\")\n",
    "          # metric.reset() reset metric states. It's typically called after the epoch completes.\n",
    "        \n",
    "        live.log_metric(f\"test/loss\", test_epoch_loss)\n",
    "        live.log_metric(f\"test/acc\", test_acc)\n",
    "        live.log_metric(f\"test/f1\", test_f1)\n",
    "        \n",
    "        metric_acc.reset()\n",
    "        metric_f1.reset()\n",
    "        \n",
    "        # Guardar el modelo si la pérdida es la mejor hasta ahora\n",
    "        if (test_f1 - best_f1) > min_delta:\n",
    "            print(\"Improved test F1 from {:.4f} to {:.4f}. Saving model...\".format(best_f1, test_f1))\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_epoch_loss,\n",
    "                'test_loss': test_epoch_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'test_f1': test_f1,\n",
    "                'train_f1': train_f1,\n",
    "            }, save_path)\n",
    "            best_acc = test_acc\n",
    "            best_f1 = test_f1\n",
    "            patience = early_stopping_patience\n",
    "            \n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "        live.next_step() #finish epoch\n",
    "    print('Finished Training')\n",
    "    dict_model = torch.load(save_path)\n",
    "    dict_results = {'epoch':dict_model['epoch'],'train_loss': dict_model['train_loss'], 'test_loss':dict_model['test_loss'], 'train_acc':dict_model['train_acc'],\n",
    "                    'test_acc':dict_model['test_acc'], 'test_f1':dict_model['test_f1'], 'train_f1':dict_model['train_f1']}\n",
    "   # model = model.load_state_dict(dict_model['model_state_dict'])\n",
    "    return  dict_results\n",
    "# save_path = f'/home/gass/audio-sensitive-content-detection/models/reprocess_data_3_chans_history_torch_baseline_convnext_femto_{IMG_SIZE}_custom_exp_{exp_num}.pth'\n",
    "# save_dict_results = f'/home/gass/audio-sensitive-content-detection/models/reprocess_data_3_chans_history_torch_baseline_convnext_femto_{IMG_SIZE}_custom_exp_{exp_num}.pkl'\n",
    "\n",
    "save_path = f'/home/gass/audio-sensitive-content-detection/models/{ESPEC}_preprocess_data_3_chans_history_torch_baseline_convnext_femto_{IMG_SIZE}_custom_exp_{exp_num}.pth'\n",
    "save_dict_results = f'/home/gass/audio-sensitive-content-detection/models/{ESPEC}_preprocess_data_3_chans_history_torch_baseline_convnext_femto_{IMG_SIZE}_custom_exp_{exp_num}.pkl'\n",
    "\n",
    "skf= StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "history_list = []\n",
    "val_acc = []\n",
    "val_f1 = []\n",
    "\n",
    "test_acc = []\n",
    "test_f1 = []\n",
    "\n",
    "from torch.utils.data import default_collate\n",
    "mixup = v2.MixUp(alpha=0.2, num_classes=NUM_CLASSES)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return mixup(*default_collate(batch))\n",
    "\n",
    "def cross_validation():\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(df_video_train, df_video_train['video_label'])):\n",
    "                \n",
    "                # Crear DataFrames para conjuntos de entrenamiento y validación\n",
    "                df_train_audios = df_video_train.iloc[train_index]\n",
    "                df_val_audios = df_video_train.iloc[val_index]\n",
    "                \n",
    "                if ESPEC == 'mfcc':\n",
    "                    if AUG:\n",
    "                        print('MFCC ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                        ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc_aug)\n",
    "                        ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_mfcc)\n",
    "                    else:\n",
    "                        print('MFCC ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                        ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc)\n",
    "                        ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_mfcc)\n",
    "                elif ESPEC == 'mel':\n",
    "                    if AUG:\n",
    "                        print('MEL ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                        ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data_train_aug)\n",
    "                        ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_data)\n",
    "                    else:\n",
    "                        print('MEL ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                        ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data)\n",
    "                        ds_val_audios =  CustomImageDataset(df_val_audios, transform=preprocess_data)\n",
    "\n",
    "                \n",
    "                dataloader_train = DataLoader(ds_train_audios, batch_size=batch_size, shuffle=True, num_workers=16, collate_fn=collate_fn)\n",
    "                dataloader_val = DataLoader(ds_val_audios, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "                \n",
    "                \n",
    "                # Crear una instancia del nuevo modelo\n",
    "                model = CustomConvNeXt(N=N)\n",
    "                # model = NewCustomCNN()\n",
    "                # Obtener el número total de parámetros entrenables\n",
    "                total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                print(f'Params: {params}')\n",
    "                print(f\"Num. of trainable parameters: {total_params}\")\n",
    "                # Definir la función de pérdida y el optimizador\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "                #optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay, momentum=0.9)\n",
    "                if scheduler_learning:\n",
    "                    #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min= 1E-4)\n",
    "                    scheduler = ExponentialLR(optimizer, gamma=GAMMA_TO_DECAY)\n",
    "                    #scheduler = None#CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=0.000001)\n",
    "                else:\n",
    "                    scheduler = None\n",
    "\n",
    "                model.to(device)\n",
    "                label_counts = df_train_audios.value_counts('label')\n",
    "                label_counts = label_counts.reindex(label_counts.index[::-1])\n",
    "                # Cambiar las etiquetas de 0 y 1\n",
    "                label_counts.index = ['Sensible' if label == 1 else 'Seguro' if label == 0 else label for label in label_counts.index]\n",
    "                class_counts = [label_counts['Seguro'], label_counts['Sensible'], ...]  # Lista con el número de muestras de cada clase\n",
    "                total_samples = sum(label_counts)\n",
    "                weights = [total_samples / count for count in label_counts]\n",
    "\n",
    "                # Multiplica los pesos por el factor de aumento\n",
    "                weights_list = [weight * f_weighted for weight in weights]\n",
    "                print(weights_list)\n",
    "                if WEIGHT:\n",
    "                    print('WEIGHTED LOSS')\n",
    "                    weight_classes = torch.tensor(weights_list).float()\n",
    "                    weight_classes = weight_classes.to(device)\n",
    "                    criterion = torch.nn.CrossEntropyLoss(weight=weight_classes, label_smoothing=label_smoothing)\n",
    "                    criterion_val = torch.nn.CrossEntropyLoss()\n",
    "                else:\n",
    "                    criterion = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "                    criterion_val = torch.nn.CrossEntropyLoss()\n",
    "                    \n",
    "                history = train_model(fold, model, dataloader_train, dataloader_val, criterion, criterion_val,optimizer, scheduler, num_epochs=num_epochs, \n",
    "                                    early_stopping_patience=early_stopping_patience,\n",
    "                                    save_path=save_path)\n",
    "\n",
    "                history_list.append(history)\n",
    "                value_val = history['val_acc']\n",
    "                value_val_f1 = history['val_f1']\n",
    "                print(f'Fold {fold} finished, Val_accuracy: {value_val}, Val_f1: {value_val_f1}')\n",
    "                val_acc.append(history['val_acc'])\n",
    "                val_f1.append(history['val_f1'])\n",
    "\n",
    "                # Guarda history_list en un archivo con Pickle\n",
    "        print('Mean val_acc:', np.mean(val_acc), 'Mean val_f1:', np.mean(val_f1))\n",
    "        with open(save_dict_results, 'wb') as f:\n",
    "                pickle.dump(history_list, f)        \n",
    "                \n",
    "        print(history_list)    \n",
    "            \n",
    "        \n",
    "def testing():\n",
    "    with Live(report=\"notebook\", exp_name=exp_name) as live:\n",
    "        df_video_train = pd.read_parquet('/home/gass/audio-sensitive-content-detection/data/dfTrainVideo.parquet')\n",
    "        df_video_test = pd.read_parquet('/home/gass/audio-sensitive-content-detection/data/dfTestVideo.parquet')\n",
    "        live.log_params(params)\n",
    "        \n",
    "  \n",
    "        # Crear DataFrames para conjuntos de entrenamiento y validación\n",
    "        df_train_videos = df_video_train\n",
    "        df_video_test = df_video_test\n",
    "        \n",
    "        # Obtener nombres de videos para los conjuntos de entrenamiento y validación\n",
    "        train_video_names = df_train_videos['video_name'].tolist()\n",
    "        val_video_names = df_video_test['video_name'].tolist()\n",
    "        \n",
    "        # Filtrar df_audio para obtener solo las filas correspondientes a los conjuntos de entrenamiento y validación\n",
    "        df_train_audios = df_audio[df_audio['filename'].str.contains('|'.join(train_video_names))]\n",
    "        df_test_audios = df_audio[df_audio['filename'].str.contains('|'.join(val_video_names))]\n",
    "        if ESPEC == 'mfcc':\n",
    "            if AUG:\n",
    "                print('MFCC ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc_aug)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mfcc)\n",
    "            else:\n",
    "                print('MFCC ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mfcc)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mfcc)\n",
    "        elif ESPEC == 'mel':\n",
    "            if AUG:\n",
    "                print('MEL ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data_train_aug)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_data)\n",
    "            else:\n",
    "                print('MEL ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_data)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_data)\n",
    "        else:\n",
    "            if AUG:\n",
    "                print('MEL/MFCC/LFCC ESPECTOGRAM EXPERIMENT AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mel_mfcc_lfcc_aug)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mel_mfcc_lfcc)\n",
    "            else:\n",
    "                print('MEL/MFCC/LFCC ESPECTOGRAM EXPERIMENT NO AUGMENTED')\n",
    "                ds_train_audios = CustomImageDataset(df_train_audios, transform=preprocess_mel_mfcc_lfcc)\n",
    "                ds_test_audios =  CustomImageDataset(df_test_audios, transform=preprocess_mel_mfcc_lfcc)\n",
    "                \n",
    "                \n",
    "        dataloader_train = DataLoader(ds_train_audios, batch_size=batch_size, shuffle=True, num_workers=16, collate_fn=collate_fn)\n",
    "        dataloader_val = DataLoader(ds_test_audios, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "        \n",
    "        \n",
    "        # Crear una instancia del nuevo modelo\n",
    "        model = CustomConvNeXt(N=N)\n",
    "        # model = NewCustomCNN()\n",
    "        # Obtener el número total de parámetros entrenables\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"Num. of trainable parameters: {total_params}\")\n",
    "        print(params)\n",
    "        # Definir la función de pérdida y el optimizador\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "        #optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay, momentum=0.9)\n",
    "        if scheduler_learning:\n",
    "            #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min= 1E-4)\n",
    "            scheduler = ExponentialLR(optimizer, gamma=GAMMA_TO_DECAY)\n",
    "            #scheduler = None#CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=0.000001)\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        model.to(device)\n",
    "        label_counts = df_train_audios.value_counts('label')\n",
    "        label_counts = label_counts.reindex(label_counts.index[::-1])\n",
    "        # Cambiar las etiquetas de 0 y 1\n",
    "        label_counts.index = ['Sensible' if label == 1 else 'Seguro' if label == 0 else label for label in label_counts.index]\n",
    "        class_counts = [label_counts['Seguro'], label_counts['Sensible'], ...]  # Lista con el número de muestras de cada clase\n",
    "        total_samples = sum(label_counts)\n",
    "        weights = [total_samples / count for count in label_counts]\n",
    "\n",
    "        # Multiplica los pesos por el factor de aumento\n",
    "        weights_list = [weight * f_weighted for weight in weights]\n",
    "        print(weights_list)\n",
    "        if WEIGHT:\n",
    "            print('WEIGHTED LOSS')\n",
    "            weight_classes = torch.tensor(weights_list).float()\n",
    "            weight_classes = weight_classes.to(device)\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weight_classes, label_smoothing=label_smoothing)\n",
    "            criterion_test = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "            criterion_test = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        history = train_model_for_testing(live, model, dataloader_train, dataloader_val, criterion, criterion_test,optimizer, scheduler, num_epochs=num_epochs, \n",
    "                            early_stopping_patience=early_stopping_patience,\n",
    "                            save_path=save_path)\n",
    "\n",
    "        history_list.append(history)\n",
    "        value_test = history['test_acc']\n",
    "        value_test_f1 = history['test_f1']\n",
    "        print(f'Finished, test_f1: {value_test_f1}, test_f1: {value_test_f1}')\n",
    "        test_acc.append(history['test_acc'])\n",
    "        test_f1.append(history['test_f1'])\n",
    "        live.log_metric(f\"test/mean_test_acc\", np.mean(test_acc))\n",
    "        live.log_metric(f\"test/mean_test_f1\", np.mean(test_f1))\n",
    "        # Guarda history_list en un archivo con Pickle\n",
    "        print('Mean val_acc:', np.mean(test_acc), 'Mean test_f1:', np.mean(test_f1))\n",
    "        with open(save_dict_results, 'wb') as f:\n",
    "                pickle.dump(history_list, f)        \n",
    "                \n",
    "        print(history_list)    \n",
    "                \n",
    "                \n",
    "#testing()\n",
    "cross_validation()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
